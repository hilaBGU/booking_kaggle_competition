{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hilak\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hilak\\anaconda3\\envs\\booking_kaggel_compatitiom\\python.exe\n",
      "C:\\Users\\hilak\\anaconda3\\python.exe\n",
      "C:\\Users\\hilak\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe\n"
     ]
    }
   ],
   "source": [
    "!where python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastparquet in c:\\users\\hilak\\anaconda3\\envs\\booking_kaggel_compatitiom\\lib\\site-packages (2024.11.0)\n",
      "Requirement already satisfied: pandas>=1.5.0 in c:\\users\\hilak\\anaconda3\\envs\\booking_kaggel_compatitiom\\lib\\site-packages (from fastparquet) (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\hilak\\anaconda3\\envs\\booking_kaggel_compatitiom\\lib\\site-packages (from fastparquet) (2.0.2)\n",
      "Requirement already satisfied: cramjam>=2.3 in c:\\users\\hilak\\anaconda3\\envs\\booking_kaggel_compatitiom\\lib\\site-packages (from fastparquet) (2.8.4rc3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hilak\\anaconda3\\envs\\booking_kaggel_compatitiom\\lib\\site-packages (from fastparquet) (2024.12.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hilak\\anaconda3\\envs\\booking_kaggel_compatitiom\\lib\\site-packages (from fastparquet) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hilak\\anaconda3\\envs\\booking_kaggel_compatitiom\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hilak\\anaconda3\\envs\\booking_kaggel_compatitiom\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hilak\\anaconda3\\envs\\booking_kaggel_compatitiom\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hilak\\anaconda3\\envs\\booking_kaggel_compatitiom\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.17.0)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\hilak\\anaconda3\\envs\\booking_kaggel_compatitiom\\lib\\site-packages (18.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install fastparquet\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "booking_competition.ipynb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyarrow'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_7904/1185380410.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpyplot\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mseaborn\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0msns\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mpyarrow\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     14\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mfastparquet\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'pyarrow'"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.sparse import hstack, save_npz, load_npz\n",
    "from tqdm import tqdm\n",
    "from joblib import dump, load\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyarrow\n",
    "import fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "print(\"Loading data...\")\n",
    "reviews = pd.read_csv('../data/train_reviews.csv')\n",
    "users = pd.read_csv('../data/train_users.csv')\n",
    "matches = pd.read_csv('../data/train_matches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing data\n",
    "print(\"Preprocessing data...\")\n",
    "reviews['review_title'] = reviews['review_title'].fillna(\"No Title\")\n",
    "reviews['review_positive'] = reviews['review_positive'].fillna(\"No Positive Review\")\n",
    "reviews['review_negative'] = reviews['review_negative'].fillna(\"No Negative Review\")\n",
    "users['guest_country'] = users['guest_country'].fillna(\"Unknown\")\n",
    "users['room_nights'] = users['room_nights'].apply(lambda x: min(x, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Merging datasets\n",
    "data = pd.merge(matches, reviews, on='review_id', how='inner')\n",
    "data = pd.merge(data, users, on='user_id', how='inner')\n",
    "data['text'] = data['review_title'] + \" \" + data['review_positive'] + \" \" + data['review_negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_1896/923440606.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Save processed data\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_parquet\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"processed_data.parquet\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Data saved as 'processed_data.parquet'\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    205\u001B[0m                 \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    206\u001B[0m                     \u001B[0mkwargs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mnew_arg_name\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnew_arg_value\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 207\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    208\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    209\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mF\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36mto_parquet\u001B[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001B[0m\n\u001B[0;32m   2675\u001B[0m         \u001B[1;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mio\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparquet\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mto_parquet\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2676\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2677\u001B[1;33m         return to_parquet(\n\u001B[0m\u001B[0;32m   2678\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2679\u001B[0m             \u001B[0mpath\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parquet.py\u001B[0m in \u001B[0;36mto_parquet\u001B[1;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001B[0m\n\u001B[0;32m    410\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpartition_cols\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    411\u001B[0m         \u001B[0mpartition_cols\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mpartition_cols\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 412\u001B[1;33m     \u001B[0mimpl\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    413\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    414\u001B[0m     \u001B[0mpath_or_buf\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mFilePathOrBuffer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mio\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mBytesIO\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mpath\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mpath\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parquet.py\u001B[0m in \u001B[0;36mget_engine\u001B[1;34m(engine)\u001B[0m\n\u001B[0;32m     51\u001B[0m                 \u001B[0merror_msgs\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;34m\"\\n - \"\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0merr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 53\u001B[1;33m         raise ImportError(\n\u001B[0m\u001B[0;32m     54\u001B[0m             \u001B[1;34m\"Unable to find a usable engine; \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m             \u001B[1;34m\"tried using: 'pyarrow', 'fastparquet'.\\n\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mImportError\u001B[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "# Save processed data\n",
    "data.to_parquet(\"processed_data.parquet\")\n",
    "print(\"Data saved as 'processed_data.parquet'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "data = pd.read_parquet(\"processed_data.parquet\")\n",
    "\n",
    "# Basic statistics\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting data by users\n",
    "unique_users = data['user_id'].unique()\n",
    "train_users, test_users = train_test_split(unique_users, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = data[data['user_id'].isin(train_users)]\n",
    "test_data = data[data['user_id'].isin(test_users)]\n",
    "\n",
    "# Save train and test splits\n",
    "train_data.to_parquet(\"../data/train_data.parquet\")\n",
    "test_data.to_parquet(\"../data/test_data.parquet\")\n",
    "print(\"Train and test data saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load train data\n",
    "train_data = pd.read_parquet(\"../data/train_data.parquet\")\n",
    "\n",
    "# Sample 20% of train data\n",
    "train_data_sample = train_data.sample(frac=0.2, random_state=42)\n",
    "\n",
    "# TF-IDF for text features\n",
    "vectorizer = TfidfVectorizer(max_features=2000)\n",
    "X_train_text = vectorizer.fit_transform(tqdm(train_data_sample['text'], desc=\"Processing Train Text Features\"))\n",
    "\n",
    "# Numeric features\n",
    "numeric_features = ['review_score', 'room_nights', 'accommodation_score', 'accommodation_star_rating',\n",
    "                    'location_is_ski', 'location_is_beach', 'location_is_city_center']\n",
    "X_train_numeric = train_data_sample[numeric_features].fillna(0).values\n",
    "\n",
    "# Combine features\n",
    "X_train = hstack([X_train_text, X_train_numeric])\n",
    "y_train = train_data_sample['user_id'].values\n",
    "\n",
    "# Save features\n",
    "save_npz('../data/X_train.npz', X_train)\n",
    "np.save('../data/y_train.npy', y_train)\n",
    "dump(vectorizer, '../data/vectorizer.joblib')\n",
    "print(\"Features and vectorizer saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "model = RandomForestClassifier(n_estimators=10, max_depth=10, n_jobs=-1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save model\n",
    "dump(model, '../data/random_forest_model.joblib')\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate MRR@10\n",
    "def calculate_mrr_at_k(y_true, y_pred, k=10):\n",
    "    reciprocal_ranks = []\n",
    "    for true_label, preds in zip(y_true, y_pred):\n",
    "        try:\n",
    "            rank = preds[:k].tolist().index(true_label) + 1\n",
    "            reciprocal_ranks.append(1 / rank)\n",
    "        except ValueError:\n",
    "            reciprocal_ranks.append(0)\n",
    "    return np.mean(reciprocal_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_data = pd.read_parquet(\"../data/test_data.parquet\")\n",
    "\n",
    "# Prepare test features\n",
    "vectorizer = load('../data/vectorizer.joblib')\n",
    "X_test_text = vectorizer.transform(tqdm(test_data['text'], desc=\"Processing Test Text Features\"))\n",
    "\n",
    "X_test_numeric = test_data[numeric_features].fillna(0).values\n",
    "X_test = hstack([X_test_text, X_test_numeric])\n",
    "y_test = test_data['user_id'].values\n",
    "\n",
    "# Save test features\n",
    "save_npz('../data/X_test.npz', X_test)\n",
    "np.save('../data/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "model = load('../data/random_forest_model.joblib')\n",
    "y_test_prob = model.predict_proba(X_test)\n",
    "\n",
    "top_k_predictions = np.argsort(y_test_prob, axis=1)[:, -10:][:, ::-1]\n",
    "mrr_at_10 = calculate_mrr_at_k(y_test, top_k_predictions)\n",
    "\n",
    "# Save evaluation results\n",
    "metrics = {'mrr_at_10': mrr_at_10}\n",
    "with open('metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f)\n",
    "\n",
    "print(f\"MRR@10: {mrr_at_10:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare submission\n",
    "submission = pd.DataFrame({\n",
    "    'accommodation_id': test_data['accommodation_id'].values,\n",
    "    'user_id': test_data['user_id'].values\n",
    "})\n",
    "\n",
    "for i in range(10):\n",
    "    submission[f'review_{i+1}'] = np.array(test_data['review_id'])[top_k_predictions[:, i]]\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Submission file saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}