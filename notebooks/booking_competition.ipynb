{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Booking Model Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "This notebook demonstrates the training and prediction stages of the model developed for the Booking Kaggle competition.\n",
    "\n",
    "#### Important Notes:\n",
    "Certain cells involve file-saving operations and have been disabled to optimize runtime for subsequent executions.\n",
    "The notebook reflects the same process as the Python scripts used during development, ensuring consistency and reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils Package\n",
    "The utils package provides essential helper functions and classes for data preprocessing, tokenization, and dataset handling. These include:\n",
    "\n",
    "##### General utilities: \n",
    "Functions for cleaning text, scaling data, and merging datasets (utils.general_utils).\n",
    "##### Loader utilities: \n",
    "Classes for efficiently loading and batching training and validation datasets (utils.loader_utils).\n",
    "##### Model utilities: \n",
    "Functions for loading pre-trained models and performing mean pooling of embeddings (utils.model_utils).\n",
    "\n",
    "This modular structure ensures consistency across scripts and simplifies the workflow for training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.data_utils import (load_data, fill_na, merge_users_reviews_by_matches, to_lower, clean_text,\n",
    "                              replace_multiple_spaces, is_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Data Processing:\n",
    "1. Load Data: Load user data for the specified split (train, val, test).\n",
    "2. Handle Missing Values: Replace missing guest_country values with \"Unknown\".\n",
    "3. Feature Engineering: Create a consolidated features_text field, combining key user and accommodation information (e.g., guest type, country, star rating).\n",
    "4. Save Processed Data: Save the processed user data to a CSV file for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_users_data(split):\n",
    "    print('Loading users data...')\n",
    "    df_users = load_data(split=split, data_type='users')\n",
    "\n",
    "    print('Fill null values...')\n",
    "    users_fill_na_params = {'guest_country': 'Unknown'}\n",
    "    for col_name, new_value in users_fill_na_params.items():\n",
    "        fill_na(df_users, col_name, new_value)\n",
    "\n",
    "    df_users['features_text'] =\\\n",
    "        'Guest type: ' + df_users['guest_type'] + '\\n' +\\\n",
    "        'Guest country: ' + df_users['guest_country'] + '\\n' +\\\n",
    "        'Month: ' + df_users['month'].astype(str) + '\\n' +\\\n",
    "        'Accommodation type: ' + df_users['accommodation_type'] + '\\n' +\\\n",
    "        'Accommodation country: ' + df_users['accommodation_country'] + '\\n' +\\\n",
    "        'Accommodation score: ' + df_users['accommodation_score'].astype(str) + '\\n' +\\\n",
    "        'Accommodation star rating: ' + df_users['accommodation_star_rating'].astype(str) + '\\n' +\\\n",
    "        'Room nights: ' + df_users['room_nights'].astype(str)\n",
    "\n",
    "    df_users.to_csv(f'../data/processed_data/processed_{split}_users.csv', index=False)\n",
    "    return df_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Data Processing:\n",
    "1. Load Data: Load review data for the specified split (train, val, test).\n",
    "2. Handle Missing Values:\n",
    " - For the train split, remove rows with missing or very short text.\n",
    " - For other splits, fill missing text fields with empty strings.\n",
    "3. Text Cleaning:\n",
    " - Convert text to lowercase.\n",
    " - Remove unwanted characters (e.g., HTML tags, URLs, and non-alphanumeric characters).\n",
    " - Replace multiple spaces with a single space.\n",
    "4. Feature Engineering: Create an all_text field consolidating review title, positive/negative content, score, and helpful votes.\n",
    "5. Language Filter (Train Only): Retain only reviews written in English.\n",
    "6. Save Processed Data: Save the processed review data to a CSV file for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reviews_data(split):\n",
    "    print('Loading reviews data...')\n",
    "    df_reviews = load_data(split=split, data_type='reviews')\n",
    "    reviews_text_columns = ['review_title', 'review_positive', 'review_negative']\n",
    "\n",
    "    if split == 'train':\n",
    "        print('Fill null values...')\n",
    "        # Drop rows where all text columns are NaN or empty\n",
    "        df_reviews = df_reviews.dropna(subset=reviews_text_columns, how='all')\n",
    "        df_reviews = df_reviews[(df_reviews['review_title'] != '') | (df_reviews['review_positive'] != '') | (df_reviews['review_negative'] != '')]\n",
    "\n",
    "        print('Drop rows with very short combined text...')\n",
    "        # Drop rows with very short combined text (e.g., fewer than 5 words)\n",
    "        df_reviews['text'] = df_reviews['review_title'] + ' ' + df_reviews['review_positive'] + ' ' + df_reviews['review_negative']\n",
    "        df_reviews = df_reviews[df_reviews['text'].str.split().str.len() > 50]\n",
    "        df_reviews.drop('text', axis=1, inplace=True)\n",
    "    else:\n",
    "        print('Fill null values...')\n",
    "        for col_name in reviews_text_columns:\n",
    "            fill_na(df_reviews, col_name, '')\n",
    "\n",
    "    print('Cleaning text...')\n",
    "    # Convert text to lowercase\n",
    "    for col_name in reviews_text_columns:\n",
    "        df_reviews = to_lower(df_reviews, col_name)\n",
    "\n",
    "    for col_name in reviews_text_columns:\n",
    "        df_reviews[col_name] = df_reviews[col_name].apply(clean_text)\n",
    "\n",
    "    for col_name in reviews_text_columns:\n",
    "        df_reviews = replace_multiple_spaces(df_reviews, col_name)\n",
    "\n",
    "    df_reviews['all_text'] = 'Review title: ' + df_reviews['review_title'] + '\\n' + \\\n",
    "                             'Review positive: ' + df_reviews['review_positive'] + '\\n' + \\\n",
    "                             'Review negative: ' + df_reviews['review_negative'] + '\\n' + \\\n",
    "                             'Review score: ' + df_reviews['review_score'].astype(str) + '\\n' + \\\n",
    "                             'Review helpful votes: ' + df_reviews['review_helpful_votes'].astype(str)\n",
    "\n",
    "    if split == 'train':\n",
    "        print('Check if text is in English...')\n",
    "        # combine all text\n",
    "        df_reviews = df_reviews[df_reviews['all_text'].apply(is_english)]\n",
    "\n",
    "    df_reviews.to_csv(f'../data/processed_data/processed_{split}_reviews.csv', index=False)\n",
    "    return df_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-Review Merging (Train and Validation Only):\n",
    "Combine user and review datasets using match files to create positive examples for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading users data...\n",
      "Fill null values...\n",
      "Loading reviews data...\n",
      "Fill null values...\n",
      "Drop rows with very short combined text...\n",
      "Cleaning text...\n",
      "Check if text is in English...\n",
      "Loading users data...\n",
      "Fill null values...\n",
      "Loading reviews data...\n",
      "Fill null values...\n",
      "Cleaning text...\n",
      "Loading users data...\n",
      "Fill null values...\n",
      "Loading reviews data...\n",
      "Fill null values...\n",
      "Cleaning text...\n"
     ]
    }
   ],
   "source": [
    "    for split in ['train', 'val', 'test']:\n",
    "        df_users = process_users_data(split)\n",
    "        df_reviews = process_reviews_data(split)\n",
    "        if split != 'test':\n",
    "            merge_users_reviews_by_matches(df_users, df_reviews, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "This section demonstrates the training process for the dual-encoder model used to match reviews and user features.\n",
    "\n",
    "Key Steps:\n",
    "\n",
    "1. Model Initialization:\n",
    "\n",
    " - Load pre-trained sentence-transformers/all-MiniLM-L6-v2 models for reviews and user features.\n",
    " - Transfer models to GPU (cuda) for efficient training.\n",
    "\n",
    "2. Dataset Preparation:\n",
    "\n",
    " - Utilize preprocessed training and validation datasets loaded via UsersReviewsTrainDataset and UsersReviewsValDataset.\n",
    " - Batch size is set to 64 for training and 128 for validation.\n",
    "\n",
    "3. Training Process:\n",
    "\n",
    " - Train the models over 100 epochs.\n",
    " - Use AdamW optimizer with a learning rate of 3e-5 for both models.\n",
    " - Apply a learning rate scheduler (ReduceLROnPlateau) to dynamically adjust learning rates based on validation loss.\n",
    "\n",
    "4. Loss Function:\n",
    "\n",
    " - Compute similarity scores between review and user feature embeddings.\n",
    " - Optimize similarity matrix against a diagonal target using Binary Cross-Entropy Loss with Logits.\n",
    "\n",
    "5. Forward Pass:\n",
    "\n",
    " - Compute embeddings for reviews and user features.\n",
    " - Aggregate token embeddings for each sequence using mean pooling.\n",
    " - Calculate similarity scores as the dot product of review and feature embeddings.\n",
    "\n",
    "6. Validation:\n",
    "\n",
    " - Evaluate model performance on the validation dataset after each epoch.\n",
    " - Use validation loss to adjust the learning rate dynamically.\n",
    "\n",
    "7. Model Saving:\n",
    "\n",
    " - Save the trained models (review encoder and feature encoder) after each epoch for checkpointing and future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from utils.loader_utils import UsersReviewsTrainDataset, UsersReviewsValDataset\n",
    "from utils.model_utils import load_model, mean_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    print('Training model...')\n",
    "    num_epochs = 100\n",
    "    batch_size = 64\n",
    "\n",
    "    # Load model\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    review_model, review_tokenizer = load_model(model_name)\n",
    "    review_model.to('cuda')\n",
    "    features_model, features_tokenizer = load_model(model_name)\n",
    "    features_model.to('cuda')\n",
    "\n",
    "    train_dataset = UsersReviewsTrainDataset(datapath='../data/processed_data/processed_train.csv',\n",
    "                                             review_tokenizer=review_tokenizer,\n",
    "                                             features_tokenizer=features_tokenizer,\n",
    "                                             batch_size=batch_size)\n",
    "    val_dataset = UsersReviewsValDataset(datapath='../data/processed_data/processed_val.csv',\n",
    "                                         review_tokenizer=review_tokenizer,\n",
    "                                         features_tokenizer=features_tokenizer,\n",
    "                                         frac=0.2,\n",
    "                                         batch_size=batch_size)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size*2, shuffle=False)\n",
    "\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {\"params\": review_model.parameters(), \"lr\": 3e-5},  # Learning rate for Network A\n",
    "        {\"params\": features_model.parameters(), \"lr\": 3e-5},   # Learning rate for Network B\n",
    "    ])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                           patience=1,\n",
    "                                                           min_lr=1e-8,\n",
    "                                                           mode='min')\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    running_loss = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        review_model.train()\n",
    "        features_model.train()\n",
    "        pb_train = tqdm(enumerate(train_dataloader), desc=f'Epoch {epoch}', total=len(train_dataloader))\n",
    "        for idx, batch_data in pb_train:\n",
    "            review_text, features_text = batch_data\n",
    "            review_text = {key: value.squeeze(0) for key, value in review_text.items()}\n",
    "            features_text = {key: value.squeeze(0) for key, value in features_text.items()}\n",
    "            batch_data = review_text, features_text\n",
    "\n",
    "            loss = forward_pass(review_model, features_model, batch_data, loss_fn)\n",
    "\n",
    "            running_loss += [loss.item()]\n",
    "            pb_train.set_postfix_str(f\"loss: {np.mean(running_loss)}\")\n",
    "\n",
    "            # this is where the magic happens\n",
    "            optimizer.zero_grad()  # reset optimizer so gradients are all-zero\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_loss = validate(review_model, features_model, val_dataloader, loss_fn)\n",
    "        scheduler.step(val_loss)\n",
    "        review_model.save_pretrained(f\"./runs/run6/{model_name.split('/')[-1]}_review_e{epoch}.pth\")\n",
    "        features_model.save_pretrained(f\"./runs/run6/{model_name.split('/')[-1]}_features_e{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(review_model, features_model, batch_data, loss_fn):\n",
    "    review_text, features_text = batch_data\n",
    "    review_text = {key: value.to('cuda') for key, value in review_text.items()}\n",
    "    features_text = {key: value.to('cuda') for key, value in features_text.items()}\n",
    "\n",
    "    review_emb = review_model(**review_text)\n",
    "    features_emb = features_model(**features_text)\n",
    "\n",
    "    review_emb = mean_pooling(review_emb, review_text['attention_mask'])\n",
    "    features_emb = mean_pooling(features_emb, features_text['attention_mask'])\n",
    "\n",
    "    # Compute similarity scores: a 32x32 matrix\n",
    "    # row[N] reflects similarity between question[N] and answers[0...31]\n",
    "    similarity_scores = review_emb @ features_emb.T\n",
    "\n",
    "    target = torch.eye(review_emb.shape[0], dtype=torch.float32, device='cuda')\n",
    "    loss = torch.nn.BCEWithLogitsLoss()(similarity_scores, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(review_model, features_model, val_dataloader, loss_fn):\n",
    "    review_model.eval()\n",
    "    features_model.eval()\n",
    "    running_loss = []\n",
    "    pb_val = tqdm(enumerate(val_dataloader), desc=f'Validation', total=len(val_dataloader))\n",
    "    for idx, batch_data in pb_val:\n",
    "        loss = forward_pass(review_model, features_model, batch_data, loss_fn)\n",
    "        running_loss += [loss.item()]\n",
    "        pb_val.set_postfix_str(f\"loss: {np.mean(running_loss)}\")\n",
    "    return np.mean(running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and Prepare Submission\n",
    "#### Overview:\n",
    "This section describes the process of generating predictions and preparing the final submission file for the Kaggle competition. The predictions involve matching reviews with user-accommodation pairs based on their embeddings.\n",
    "\n",
    "### Key Steps:\n",
    "1. Load Models and Tokenizers:\n",
    "\n",
    " - Load pre-trained models for reviews and user features (sentence-transformers/all-MiniLM-L6-v2).\n",
    " - Load tokenizers corresponding to the models.\n",
    "\n",
    "2. Load Processed Data:\n",
    "\n",
    " - Load test user and review datasets (processed_test_users.csv and processed_test_reviews.csv).\n",
    " - Index the data by accommodation_id for efficient filtering.\n",
    "\n",
    "3. Batch Prediction:\n",
    "\n",
    " - Iterate over accommodations in batches of size N.\n",
    " - For each batch:\n",
    "     - Tokenize and encode review and user feature data.\n",
    "     - Pass the encoded data through their respective models to generate embeddings.\n",
    "     - Use mean pooling to compute sequence embeddings.\n",
    "\n",
    "4. Similarity Computation:\n",
    "\n",
    " - Compute similarity scores between user-accommodation embeddings and review embeddings.\n",
    " - Identify the top 10 most relevant reviews for each user-accommodation pair.\n",
    "\n",
    "5. Prepare Submission File:\n",
    "\n",
    " - Append results to the submission file, ensuring each row contains:\n",
    "     - accommodation_id\n",
    "     - user_id\n",
    "     - The IDs of the top 10 matching reviews (or placeholders if fewer than 10 reviews are available).\n",
    "\n",
    "6. Optimize Resource Usage:\n",
    "\n",
    " - Clear GPU memory (torch.cuda.empty_cache()) after processing each batch to ensure efficient resource usage.\n",
    "\n",
    "7. Finalize Submission:\n",
    "\n",
    " - Add a unique ID column to the submission file.\n",
    " - Save the final submission as a CSV file in the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "from utils.model_utils import mean_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_submission(user_accommodation_embeddings, review_embeddings, users_df, reviews_df):\n",
    "    submission = []\n",
    "\n",
    "    for idx, row in users_df.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        accommodation_id = row['accommodation_id']\n",
    "\n",
    "        # Get the corresponding embedding\n",
    "        user_accommodation_embedding = user_accommodation_embeddings[idx]  # Shape: (embedding_dim,)\n",
    "\n",
    "        # Find all reviews for this accommodation\n",
    "        relevant_reviews = reviews_df[reviews_df['accommodation_id'] == accommodation_id]\n",
    "\n",
    "        if relevant_reviews.empty:\n",
    "            continue  # No reviews for this accommodation\n",
    "\n",
    "        # Encode only the relevant reviews\n",
    "        relevant_review_embeddings = review_embeddings[relevant_reviews.index]  # Shape: (num_reviews_for_accommodation, embedding_dim)\n",
    "\n",
    "        # Compute similarity scores\n",
    "        similarity_scores = (relevant_review_embeddings @ user_accommodation_embedding.T).detach().cpu().numpy()\n",
    "\n",
    "        # Get top 10 most similar reviews\n",
    "        top_review_indices = similarity_scores.argsort()[-10:][::-1]  # Get top 10 indices\n",
    "\n",
    "        # Retrieve the corresponding review IDs\n",
    "        top_review_ids = relevant_reviews.iloc[top_review_indices]['review_id'].tolist()\n",
    "\n",
    "        # Ensure we have exactly 10 reviews (fill with -1 if not enough)\n",
    "        while len(top_review_ids) < 10:\n",
    "            top_review_ids.append(-1)  # Placeholder for missing reviews\n",
    "\n",
    "        # Store result\n",
    "        submission.append([accommodation_id, user_id] + top_review_ids)\n",
    "\n",
    "    # **Move DataFrame creation & return OUTSIDE the loop**\n",
    "    submission_df = pd.DataFrame(submission, columns=['accommodation_id', 'user_id'] + [f\"review_{i+1}\" for i in range(10)])\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_review(sample, review_tokenizer):\n",
    "    all_text = review_tokenizer(sample['all_text'],\n",
    "                                padding='max_length',\n",
    "                                truncation=True,\n",
    "                                return_tensors='pt',\n",
    "                                max_length=review_tokenizer.model_max_length)\n",
    "    all_text.data['input_ids'] = all_text.data['input_ids'].squeeze()\n",
    "    all_text.data['attention_mask'] = all_text.data['attention_mask'].squeeze()\n",
    "    all_text.data['token_type_ids'] = all_text.data['token_type_ids'].squeeze()\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features_text(sample, features_tokenizer):\n",
    "    features = features_tokenizer(sample['features_text'],\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  return_tensors='pt',\n",
    "                                  max_length=features_tokenizer.model_max_length)\n",
    "    features.data['input_ids'] = features.data['input_ids'].squeeze()\n",
    "    features.data['attention_mask'] = features.data['attention_mask'].squeeze()\n",
    "    features.data['token_type_ids'] = features.data['token_type_ids'].squeeze()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_batch_encodings(batch_encodings):\n",
    "    # Extract and stack 'input_ids'\n",
    "    input_ids = torch.stack([enc['input_ids'].squeeze(0) for enc in batch_encodings.values()])\n",
    "\n",
    "    # Extract and stack 'attention_mask'\n",
    "    attention_mask = torch.stack([enc['attention_mask'].squeeze(0) for enc in batch_encodings.values()])\n",
    "\n",
    "    # Optional: Handle 'token_type_ids' if present\n",
    "    if \"token_type_ids\" in batch_encodings[0]:\n",
    "        token_type_ids = torch.stack([enc['token_type_ids'].squeeze(0) for enc in batch_encodings.values()])\n",
    "    else:\n",
    "        token_type_ids = None\n",
    "\n",
    "    # Return a dictionary for the batch\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict():\n",
    "    start = 0\n",
    "    N = 10\n",
    "    counter = 1\n",
    "    save_path = \"./submissions/best_submission/submission.csv\"\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    review_model = AutoModel.from_pretrained(f\"./runs/best_run/{model_name.split('/')[-1]}_review.pth\")\n",
    "    review_model.to('cuda')\n",
    "    review_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    features_model = AutoModel.from_pretrained(f\"./runs/best_run/{model_name.split('/')[-1]}_features.pth\")\n",
    "    features_model.to('cuda')\n",
    "    features_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    users_df = pd.read_csv('../data/processed_data/processed_test_users.csv')\n",
    "    reviews_df = pd.read_csv('../data/processed_data/processed_test_reviews.csv')\n",
    "    accommodation_ids = users_df['accommodation_id'].unique()\n",
    "    users_indexed = users_df.set_index(\"accommodation_id\")\n",
    "    reviews_indexed = reviews_df.set_index(\"accommodation_id\")\n",
    "\n",
    "    for i in tqdm(range(start, len(accommodation_ids), N)):\n",
    "        filtered_acc_ids = accommodation_ids[i:i + N]\n",
    "        filtered_users_df = users_indexed.loc[filtered_acc_ids]\n",
    "        filtered_users_df = filtered_users_df.reset_index()\n",
    "        filtered_reviews_df = reviews_indexed.loc[filtered_acc_ids]\n",
    "        filtered_reviews_df = filtered_reviews_df.reset_index()\n",
    "\n",
    "        review_input = filtered_reviews_df.apply(lambda x: prepare_review(x, review_tokenizer), axis=1)\n",
    "        features_input = filtered_users_df.apply(lambda x: prepare_features_text(x, features_tokenizer), axis=1)\n",
    "\n",
    "        review_input = {key: value.to('cuda') for key, value in review_input.items()}\n",
    "        features_input = {key: value.to('cuda') for key, value in features_input.items()}\n",
    "\n",
    "        review_input = stack_batch_encodings(review_input)\n",
    "        features_input = stack_batch_encodings(features_input)\n",
    "\n",
    "        review_emb = review_model(**review_input)\n",
    "        features_emb = features_model(**features_input)\n",
    "\n",
    "        review_emb = mean_pooling(review_emb, review_input['attention_mask'])\n",
    "        features_emb = mean_pooling(features_emb, features_input['attention_mask'])\n",
    "\n",
    "        submission_df = prepare_submission(features_emb, review_emb, filtered_users_df,\n",
    "                                           filtered_reviews_df)\n",
    "        submission_df.to_csv(\n",
    "            save_path,\n",
    "            mode='a',\n",
    "            header=True if counter == 1 else False,\n",
    "            index=False)\n",
    "        counter += 1\n",
    "\n",
    "        del features_input, review_input, features_emb, review_emb  # Delete variables no longer needed\n",
    "        torch.cuda.empty_cache()  # Release unused GPU memory\n",
    "\n",
    "    submission_df = pd.read_csv(save_path)\n",
    "    if 'ID' not in submission_df.columns:\n",
    "        submission_df.insert(0, 'ID', range(1, len(submission_df) + 1))\n",
    "    submission_df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
